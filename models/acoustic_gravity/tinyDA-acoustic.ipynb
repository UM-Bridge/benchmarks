{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion for a coupled acoustic-gravity system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running `pip install tinyda ray' \n",
    "\n",
    "### Problem setup\n",
    "\n",
    "Let $\\varphi$ be the potential for the flow velocity, it is solution to \n",
    "\\begin{align}\\label{eq:forwardmodel}\n",
    "    \\frac{\\partial^2 \\varphi}{\\partial t^2}\n",
    "    - c^2 \\Delta \\varphi \n",
    "    + g \\frac{\\partial \\varphi}{\\partial z}\n",
    "    = 0,  \\quad \\text{ in } \\Omega,\n",
    "    \\\\\n",
    "    \\frac{\\partial^2 \\varphi}{\\partial t^2}\n",
    "    + g \\frac{\\partial \\varphi}{\\partial z} \n",
    "    = 0, \\quad \\text{ on } \\Gamma_s,\n",
    "    \\\\\n",
    "    \\nabla \\varphi \\cdot {\\bf n}\n",
    "    = u_b, \\quad \\text{ on } \\Gamma_b,\n",
    "\\end{align}\n",
    "where $c$ is the (constant) sound speed, $g$ is the gravity acceleration, ${\\bf n}$ is the outwards unit vector normal to the seabed $\\Gamma_b$ and $u_b$ is the seabed velocity.\n",
    "The fluid displacement $u$ is related to the potential by $u = \\nabla \\varphi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from scipy.stats import multivariate_normal, uniform, norm, rv_continuous, wasserstein_distance\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import tinyDA as tda\n",
    "import umbridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising UM-Bridge\n",
    "Before running the following cell, the model Docker image must be started from a system terminal using\n",
    "\n",
    "`docker run -it -p 4242:4242 acousticgravity`\n",
    "\n",
    "The Dockerfile can be found in the acousticgracity branch on UM-Bridge benchmarks. If not just running on a local machine replace `localhost` with the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config data\n",
    "#### Change captor location and number\n",
    "# Each element of the list \"captors\" is a tuple of (x,z) coordinates. \n",
    "# // version without PML: x should be between 6 and 36, z should be between 0 and (strictly less than) 1. \n",
    "# // version with PML: x should be between 1 and 29, z should be between 0 and (strictly less than) 1. \n",
    "captors = [(10,0.5), (18,0.9)] #[(28,0.5), (20,0.9)]\n",
    "\n",
    "#### do not change this part\n",
    "captors = np.array(captors, dtype=[('x',float),('y',float)])\n",
    "captors = np.sort(captors, order=['x','y'])\n",
    "fixedFloor= np.array([0,1]) # np.linspace(0,5,6)\n",
    "Lx, Nx = 30, 150\n",
    "Lz, Nz = 1, 10\n",
    "for i in range(len(captors)):\n",
    "    xCapt = captors[i][0]*Nx/Lx\n",
    "    fixedFloor = np.concatenate((fixedFloor, [xCapt-2, xCapt-1, xCapt, xCapt+1, xCapt+2]))\n",
    "fixedFloor=np.concatenate((fixedFloor, np.array([Nx-2,Nx-1])))  #np.concatenate((fixedFloor, np.linspace(37,42,6)))\n",
    "fixedFloor=np.unique(fixedFloor)\n",
    "fixedFloor=fixedFloor.tolist()\n",
    "captors = captors.tolist()\n",
    "print(fixedFloor)\n",
    "\n",
    "\n",
    "\n",
    "def fill_input(_input, fixedFloor):\n",
    "    full = np.zeros(Nx)\n",
    "    counter = 0\n",
    "    for i in range(len(full)):\n",
    "        if i not in fixedFloor: \n",
    "            full[i] = _input[counter]\n",
    "            counter +=1 \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the UM-Bridge model.\n",
    "umbridge_model = umbridge.HTTPModel('http://localhost:4242', \"forward\")\n",
    "\n",
    "# wrap the UM-Bridge model in the tinyDA UM-Bridge interface.\n",
    "#config={\"fixedFloor\":[0,1,2,3,4,5,26,27,28,29,30,36,37,38,39,40,41,42]} \n",
    "config={\"fixedFloor\":fixedFloor, \"captors\":captors}\n",
    "my_model = tda.UmBridgeModel(umbridge_model, umbridge_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = umbridge_model.get_input_sizes(config)[0] #the input is the value of f(x) at each grid point\n",
    "ny = umbridge_model.get_output_sizes(config)[0] #the outout is a time series of the pressure\n",
    "print(f\"input:{nx}, output:{ny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of input creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 5.0\n",
    "eps = 1e-6\n",
    "decay_rate = 10\n",
    "\n",
    "x = np.linspace(0, nx - 1, nx).reshape(-1, 1)\n",
    "\n",
    "def rbf_covariance(grid, length_scale, variance=1.0):\n",
    "    dists = cdist(grid, grid, 'euclidean')\n",
    "    return variance * np.exp(-0.5 * (dists / length_scale) ** 2)\n",
    "\n",
    "cov = rbf_covariance(x, length_scale)\n",
    "cov += eps * np.eye(nx)\n",
    "mean = np.zeros(nx)\n",
    "my_prior = multivariate_normal(mean=mean, cov=cov)\n",
    "\n",
    "# def smooth_cut(x, x0, width, k):\n",
    "#     step1 = 1 / (1 + np.exp(-k * (x - (x0 - width / 2))))\n",
    "#     step2 = 1 / (1 + np.exp(-k * (x - (x0 + width / 2))))\n",
    "#     return step1 - step2\n",
    "\n",
    "# cut = smooth_cut(x.flatten(), nx / 2, nx * (1 - 14 / nx), decay_rate)\n",
    "\n",
    "sample = my_prior.rvs()\n",
    "\n",
    "def restriction(sampin):\n",
    "    field_cut = sampin #* cut\n",
    "    return (field_cut > eps).astype(int)\n",
    "\n",
    "sample = restriction(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.set_xticks([i for i in range(0,nx,10)])\n",
    "ax.grid()\n",
    "\n",
    "ax.plot(sample,label='sample')\n",
    "#ax.plot(cut,label='smooth cut')\n",
    "#ax.plot(sample * cut, label='cut sample')\n",
    "ax.legend()\n",
    "ax.set_title(\"Binary Level Set Sample from Gaussian Random Field\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data for input 0.5\n",
    "# Input is a 15km wide fault centered around x\n",
    "# Domain size is 150km\n",
    "exact = np.zeros(nx)\n",
    "# Should be between 0 and Nx\n",
    "exact[60:80]=1\n",
    "print(exact)\n",
    "d_true = my_model(exact)\n",
    "\n",
    "# add some noise to the model output\n",
    "sigma_noise = 0.01\n",
    "d = d_true + np.random.normal(loc=0, scale=sigma_noise, size=ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(24,2), sharex=True)\n",
    "fig.suptitle('Input')\n",
    "x=np.linspace(0,nx, nx)\n",
    "t=np.linspace(0,25, int(ny/2))\n",
    "\n",
    "d_true1 = np.array(d_true)[0:ny:2]\n",
    "d_true2 = np.array(d_true)[1:ny:2]\n",
    "\n",
    "ax[0].set_title('Captor 1')\n",
    "ax[0].plot(t,d_true1)\n",
    "ax[0].plot(t,d[::2])\n",
    "\n",
    "ax[1].set_title('Captor 2')\n",
    "ax[1].plot(t,d_true2)\n",
    "ax[1].plot(t,d[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot several output and the exact (with/without noise) input/output\n",
    "\n",
    "# exact (with/without noise) input/output \n",
    "xCapt = np.array(captors)[:,0]\n",
    "yCapt = np.array(captors)[:,1]\n",
    "print(xCapt, yCapt)\n",
    "\n",
    "# Plot exact input\n",
    "figIn, axIn = plt.subplots(figsize=(24,4))\n",
    "\n",
    "axIn.plot(xCapt,yCapt,'x')\n",
    "exact_full = fill_input(exact, fixedFloor)\n",
    "axIn.plot(np.linspace(0,Nx,Nx), exact_full)\n",
    "#axIn.set_xticks([i for i in range(0,30)])\n",
    "axIn.set_title(\"Exact inputs\")\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(ncols=2, figsize=(24,4))\n",
    "# t = np.linspace(0,10,int(ny/2))\n",
    "# d_true1 = np.array(d_true)[0:ny:2]\n",
    "# d_true2 = np.array(d_true)[1:ny:2]\n",
    "\n",
    "# d1 = np.array(d)[0:ny:2]\n",
    "# d2 = np.array(d)[1:ny:2]\n",
    "\n",
    "# ax[0].set_title('Pressure, captor 1')\n",
    "# ax[0].plot(t,d_true1, label='Noise-free input')\n",
    "# ax[0].plot(t,d1, label='Noisy input')\n",
    "# ax[0].legend()\n",
    "\n",
    "# ax[1].set_title('Pressure, captor 2')\n",
    "# ax[1].plot(t,d_true2, label='Noise-free input')\n",
    "# ax[1].plot(t,d2, label='Noisy input')\n",
    "# ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot several output and the exact (with/without noise) input/output\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(24,4))\n",
    "figIn, axIn = plt.subplots(nrows=2, figsize=(24,4))\n",
    "\n",
    "# exact (with/without noise) input/output \n",
    "xCapt = np.array(captors)[:,0]*Nx/Lx\n",
    "yCapt = np.array(captors)[:,1]*Nz/Nz\n",
    "print(xCapt, yCapt)\n",
    "\n",
    "axIn[0].plot(xCapt,yCapt,'x')\n",
    "exact_full = fill_input(exact, fixedFloor)\n",
    "axIn[0].plot(exact_full)\n",
    "axIn[0].set_title(\"Exact inputs\")\n",
    "\n",
    "t = np.linspace(0,10,int(ny/2))\n",
    "d_true1 = np.array(d_true)[0:ny:2]\n",
    "d_true2 = np.array(d_true)[1:ny:2]\n",
    "\n",
    "d1 = np.array(d)[0:ny:2]\n",
    "d2 = np.array(d)[1:ny:2]\n",
    "\n",
    "ax[0][0].set_title('Pressure, captor 1')\n",
    "ax[0][0].plot(t,d_true1, label='Noise-free input')\n",
    "ax[0][0].plot(t,d1, label='Noisy input')\n",
    "ax[0][0].legend()\n",
    "\n",
    "ax[0][1].set_title('Pressure, captor 2')\n",
    "ax[0][1].plot(t,d_true2, label='Noise-free input')\n",
    "ax[0][1].plot(t,d2, label='Noisy input')\n",
    "ax[0][1].legend()\n",
    "\n",
    "\n",
    "# Plot several random samples\n",
    "samples = []\n",
    "inputs=[]\n",
    "for x in range(3):\n",
    "     log = my_prior.rvs()\n",
    "     log = restriction(log)\n",
    "     d_sample = my_model(log)\n",
    "\n",
    "     d_sample1 = np.array(d_sample)[0:ny:2]\n",
    "     d_sample2 = np.array(d_sample)[1:ny:2]\n",
    "     \n",
    "     ax[1][0].plot(t, d_sample1, label=f'samples {x}')\n",
    "     ax[1][1].plot(t, d_sample2, label=f'samples {x}')\n",
    "\n",
    "     log_full = fill_input(log,fixedFloor)\n",
    "     axIn[1].plot(range(len(log_full)), log_full, label=f'input {x}')\n",
    "     samples.append(d_sample)\n",
    "     inputs.append(log)\n",
    "\n",
    "ax[1][0].legend()\n",
    "ax[1][1].legend()\n",
    "axIn[1].set_title(\"Random inputs\")\n",
    "axIn[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up log-likelihood\n",
    "\n",
    "Test standard Gaussian log-likelihood:\n",
    "$$\\log ⁡L(x)=-0.5(x-y)^T\\Sigma^{-1}(x−y)$$\n",
    "\n",
    "with a Wasserstein-2-based log-likelihood as in Dunlop+Yang, 2021:\n",
    "$$\\log ⁡L(x)\\propto -\\lambda W_2(x,y),$$\n",
    "\n",
    "where:\n",
    "- x = model output\n",
    "- y = ground truth\n",
    "- λ = scale parameter (e.g. inverse covariance)\n",
    "\n",
    "While $W_1$ is contained in scipy.stats it seems $W_2$ is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WassersteinLoglike:\n",
    "    \"\"\"\n",
    "    Wasserstein-2 based log-likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, lam=1.0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            ground truth.\n",
    "        lam : float\n",
    "            Scaling factor (like inverse variance).\n",
    "        \"\"\"\n",
    "        self.data = np.sort(data)\n",
    "        self.lam = lam\n",
    "\n",
    "    def loglike(self, x):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood using squared W₂ distance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Model output (same shape as data).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Log-likelihood (unnormalized).\n",
    "        \"\"\"\n",
    "        x_sorted = np.sort(x)\n",
    "        return -self.lam * np.mean((x_sorted - self.data) ** 2)\n",
    "\n",
    "    def grad_loglike(self, x):\n",
    "        \"\"\"\n",
    "        Gradient of the W₂ log-likelihood.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Gradient of the log-likelihood.\n",
    "        \"\"\"\n",
    "        n = len(x)\n",
    "        sort_idx = np.argsort(x)\n",
    "        x_sorted = x[sort_idx]\n",
    "        y_sorted = self.data\n",
    "\n",
    "        grad_sorted = -2 * self.lam * (x_sorted - y_sorted) / n\n",
    "\n",
    "        grad = np.zeros_like(x)\n",
    "        grad[sort_idx] = grad_sorted\n",
    "        return grad\n",
    "    \n",
    "# Test for Gaussian log like\n",
    "#sigma = 2.0\n",
    "#cov_likelihood = sigma**2*np.eye(d_true.shape[0])\n",
    "#my_loglike = tda.GaussianLogLike(d_true, cov_likelihood)\n",
    "\n",
    "# Test for Wasserstain based log like\n",
    "# TODO move implementation into tinyDA\n",
    "my_loglike = WassersteinLoglike(d_true, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true \", np.exp(my_loglike.loglike(d_true)))\n",
    "print(\"true + small noise \", np.exp(my_loglike.loglike(d)))\n",
    "\n",
    "for d_sample in samples:\n",
    "    print(np.exp(my_loglike.loglike(d_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the Posterior\n",
    "def levelset_model(parameters):\n",
    "    levelset_params = restriction(parameters)\n",
    "    return my_model(levelset_params)\n",
    "\n",
    "my_posterior = tda.Posterior(my_prior, my_loglike, levelset_model)\n",
    "#my_posterior = tda.Posterior(my_prior, my_loglike, my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Crank-Nicolson with cutoff and levelset\n",
    "class CrankNicolson(tda.CrankNicolson):\n",
    "    def __init__(self, scaling, adaptive):\n",
    "        super().__init__(scaling=scaling, adaptive=adaptive)\n",
    "\n",
    "    def make_proposal(self, link):\n",
    "        # make a pCN proposal.\n",
    "        self.scaling = min(self.scaling, 1.0 - 1e-3)\n",
    "        return np.sqrt(\n",
    "            1 - self.scaling**2\n",
    "        ) * link.parameters + self.scaling * np.random.multivariate_normal(\n",
    "            self._mean, self.C\n",
    "        )\n",
    "\n",
    "# preconditioned Crank-Nicolson\n",
    "pcn_scaling = 0.15\n",
    "pcn_adaptive = True\n",
    "my_proposal = CrankNicolson(scaling=pcn_scaling, adaptive=pcn_adaptive)\n",
    "\n",
    "# # random walk Metropolis\n",
    "#rwmh_cov = np.eye(nx)\n",
    "#rmwh_scaling = 0.1\n",
    "#rwmh_adaptive = False\n",
    "#my_proposal = tda.GaussianRandomWalk(C=rwmh_cov, scaling=rmwh_scaling, adaptive=rwmh_adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes, iteration number is small for the given problem; Choose a larger number for real applications.\n",
    "my_chains = tda.sample(my_posterior, my_proposal, iterations=6000, n_chains=2, force_sequential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tinyDA chains to an ArViz InferenceData object.\n",
    "burnin = 1000\n",
    "idata = tda.to_inference_data(my_chains, burnin=burnin)\n",
    "az.to_netcdf(idata, \"results_wassersteinlog.nc\") # to store\n",
    "idata = az.from_netcdf(\"results_wassersteinlog.nc\")# to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idata)\n",
    "print(idata.sample_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display posterior summary statistics.\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "info = az.summary(idata);\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior kernel densities and traces.\n",
    "az.plot_trace(idata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the parameters from the chains.\n",
    "subsampling_rate = 20\n",
    "parameters = [link.parameters for link in my_chains['chain_0'][burnin::subsampling_rate] + my_chains['chain_1'][burnin::subsampling_rate]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "ids = np.random.randint(0, len(parameters), n_samples)\n",
    "\n",
    "# Evaluate model outputs and likelihoods\n",
    "likelihoods = []\n",
    "outputs = []\n",
    "\n",
    "for i in ids:\n",
    "    param = parameters[i]\n",
    "    output = levelset_model(param)\n",
    "    outputs.append(output)\n",
    "    idata_sub = idata.sel(draw=slice(None, None, subsampling_rate))\n",
    "    loglikes = idata_sub.sample_stats[\"likelihood\"].values.flatten()\n",
    "    #print(\"Likelihood \", loglikes[i])\n",
    "    likelihoods.append(loglikes[i])\n",
    "\n",
    "# Normalize likelihoods for colormap\n",
    "likelihoods = np.array(likelihoods)\n",
    "norm = colors.Normalize(vmin=np.min(likelihoods), vmax=np.max(likelihoods))\n",
    "cmap = cm.viridis  # Or any matplotlib colormap\n",
    "colors_list = [cmap(norm(l)) for l in likelihoods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "\n",
    "d_true = np.array(d_true)\n",
    "ax[0].plot(d_true[::2], 'k--', label=\"True output 1\")\n",
    "ax[1].plot(d_true[1::2], 'k--', label=\"True output 2\")\n",
    "for output in outputs:\n",
    "    ax[0].plot(output[::2], alpha=0.03)\n",
    "    ax[1].plot(output[1::2], alpha=0.03)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "\n",
    "# Evaluate model outputs and likelihoods\n",
    "likelihoods = []\n",
    "outputs = []\n",
    "\n",
    "for ids in range(n_samples):\n",
    "    log = my_prior.rvs()\n",
    "    log = restriction(log)\n",
    "    output = levelset_model(log)\n",
    "    outputs.append(output)\n",
    "    idata_sub = idata.sel(draw=slice(None, None, subsampling_rate))\n",
    "    loglikes = idata_sub.sample_stats[\"likelihood\"].values.flatten()\n",
    "    likelihoods.append(loglikes[i])\n",
    "\n",
    "# Normalize likelihoods for colormap\n",
    "likelihoods = np.array(likelihoods)\n",
    "norm = colors.Normalize(vmin=np.min(likelihoods), vmax=np.max(likelihoods))\n",
    "cmap = cm.viridis  # Or any matplotlib colormap\n",
    "colors_list = [cmap(norm(l)) for l in likelihoods]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "\n",
    "d_true = np.array(d_true)\n",
    "ax[0].plot(d_true[::2], 'k--', label=\"True output 1\")\n",
    "ax[1].plot(d_true[1::2], 'k--', label=\"True output 2\")\n",
    "for output in outputs:\n",
    "    ax[0].plot(output[::2], alpha=0.03)\n",
    "    ax[1].plot(output[1::2], alpha=0.03)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "\n",
    "all_links = my_chains['chain_0'][burnin::subsampling_rate] + my_chains['chain_1'][burnin::subsampling_rate]\n",
    "sorted_links = sorted(all_links, key=lambda l: -l.likelihood)\n",
    "top_links = sorted_links[:5]\n",
    "\n",
    "d_true = np.array(d_true)\n",
    "ax[0].plot(d_true[::2], 'k--', label=\"True output 1\")\n",
    "ax[1].plot(d_true[1::2], 'k--', label=\"True output 2\")\n",
    "\n",
    "for link in top_links:\n",
    "    output = np.array(levelset_model(link.parameters))\n",
    "    ax[0].plot(output[::2], alpha=0.3, label=f\"loglike: {link.likelihood:.2f}\")\n",
    "    ax[1].plot(output[1::2], alpha=0.3, label=f\"loglike: {link.likelihood:.2f}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(0,nx, nx)\n",
    "t=np.linspace(0,25, int(ny/2))\n",
    "d1 = np.array(d_true)[0:ny:2]\n",
    "d2 = np.array(d_true)[1:ny:2]\n",
    "\n",
    "output = levelset_model(info['mean'])\n",
    "o1 = np.array(output)[0:ny:2]\n",
    "o2 = np.array(output)[1:ny:2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,2))\n",
    "ax.plot(t,d1, label='True output 1')\n",
    "ax.plot(t,o1, label='mean output 1')\n",
    "ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,2))\n",
    "ax.plot(t,d2, label='True output 2')\n",
    "ax.plot(t,o2, '--' ,label='mean output 2')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(info))\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(exact)\n",
    "plt.title('Exact input')\n",
    "#plt.plot(x, restriction(info['mean']), 'o')  # x = index, y = mean\n",
    "#plt.plot(x, info['mean'], 'x')  # x = index, y = mean\n",
    "#print(top_links[0])\n",
    "plt.plot(x, restriction(top_links[0].parameters), 'x')  # x = index, y = mean\n",
    "plt.plot([captors[0][0]*Nx/Lx,captors[1][0]*Nx/Lx],[0,0],'x')\n",
    "plt.ylabel('mean')\n",
    "plt.title('Posterior Means')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_rank(idata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Monte Carlo\n",
    "\n",
    "Adjustments:\n",
    "- replace manual logging with arvis inference structure (easier/faster saving in netcdf format, immediate access to plotting routines)\n",
    "- replace prior class with scipy priors (compatibility with tinyDA code above)\n",
    "- replace loglikelihood class with tinyDA style loglikelihood\n",
    "- replace MCMC steps with tinyDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Mon Jul 28 19:00:22 2025\n"
     ]
    }
   ],
   "source": [
    "from os import sys, path, getcwd\n",
    "sys.path.append(path.dirname(getcwd()))\n",
    "\n",
    "import functools as ft\n",
    "\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "import scipy.stats as stats\n",
    "\n",
    "from SMC import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the example provided before moving on to more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth =  [[3.32142388]]\n",
      "Noiseless data [[3.0053486  2.46057133 2.01454541 1.49241195 1.22188356]]\n",
      "Data [[2.94114336 2.39636608 1.95034017 1.42820671 1.15767832]]\n",
      "SMC for RESULTS/SMC_M_1000_acousticgravity - Started: Mon Jul 28 19:00:23 2025\n",
      "------ TEMPERING-STEP 0 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 0.0009765625\n",
      ">> RESAMPLED (ESS 85.95% , exponent < 1)\n",
      "\n",
      "--- duplicates: 344/1000  |  highest count: 2x\n",
      "Applying MCMC kernel 4 times \n",
      "\n",
      "Acceptance ratio: 0.675 >0.3 \n",
      "  => variance scaling: 1.0->2.0\n",
      ">> MCMC complete!\n",
      "--- duplicates: 0/1000  |  highest count: 1x\n",
      "[saved intermediate result]\n",
      "Tempering steps done:  0\n",
      "------ TEMPERING-STEP 1 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 0.0039033889770507812\n",
      ">> RESAMPLED (ESS 72.3% , exponent < 1)\n",
      "\n",
      "--- duplicates: 572/1000  |  highest count: 2x\n",
      "Applying MCMC kernel 4 times \n",
      "\n",
      "Acceptance ratio: 0.2805 ~ 0.25 \n",
      "  => keep variance scaling: 2.0\n",
      ">> MCMC complete!\n",
      "--- duplicates: 52/1000  |  highest count: 2x\n",
      "[saved intermediate result]\n",
      "Tempering steps done:  1\n",
      "------ TEMPERING-STEP 2 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 0.012658144347369671\n",
      ">> RESAMPLED (ESS 74.26% , exponent < 1)\n",
      "\n",
      "--- duplicates: 547/1000  |  highest count: 4x\n",
      "Applying MCMC kernel 4 times \n",
      "\n",
      "Acceptance ratio: 0.16375 ~ 0.25 \n",
      "  => keep variance scaling: 2.0\n",
      ">> MCMC complete!\n",
      "--- duplicates: 179/1000  |  highest count: 3x\n",
      "[saved intermediate result]\n",
      "Tempering steps done:  2\n",
      "------ TEMPERING-STEP 3 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 0.03869157218196051\n",
      ">> RESAMPLED (ESS 74.21% , exponent < 1)\n",
      "\n",
      "--- duplicates: 614/1000  |  highest count: 5x\n",
      "Applying MCMC kernel 4 times \n",
      "\n",
      "Acceptance ratio: 0.09975 <0.15 \n",
      "  => variance scaling: 2.0->1.0\n",
      ">> MCMC complete!\n",
      "--- duplicates: 348/1000  |  highest count: 5x\n",
      "[saved intermediate result]\n",
      "Tempering steps done:  3\n",
      "------ TEMPERING-STEP 4 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 0.1147325708667859\n",
      ">> RESAMPLED (ESS 75.14% , exponent < 1)\n",
      "\n",
      "--- duplicates: 706/1000  |  highest count: 9x\n",
      "Applying MCMC kernel 4 times \n",
      "\n",
      "Acceptance ratio: 0.09925 <0.15 \n",
      "  => variance scaling: 1.0->0.5\n",
      ">> MCMC complete!\n",
      "--- duplicates: 409/1000  |  highest count: 9x\n",
      "[saved intermediate result]\n",
      "Tempering steps done:  4\n",
      "------ TEMPERING-STEP 5 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 0.4060754650248847\n",
      ">> RESAMPLED (ESS 75.0% , exponent < 1)\n",
      "\n",
      "--- duplicates: 713/1000  |  highest count: 14x\n",
      "Applying MCMC kernel 8.0 times \n",
      "\n",
      "Acceptance ratio: 0.09675 <0.15 \n",
      "  => variance scaling: 0.5->0.25\n",
      ">> MCMC complete!\n",
      "--- duplicates: 283/1000  |  highest count: 14x\n",
      "[saved intermediate result]\n",
      "Tempering steps done:  5\n",
      "------ TEMPERING-STEP 6 --------------------------\n",
      "\n",
      ">> REWEIGHTED with exponent 1.0\n",
      ">> NOT resampled (ESS 84.36% , exponent = 1)\n",
      "\n",
      "--- duplicates: 283/1000  |  highest count: 14x\n",
      "Applying MCMC kernel 32.0 times \n",
      "\n",
      "Acceptance ratio: 0.15215625 ~ 0.25 \n",
      "  => keep variance scaling: 0.25\n",
      ">> MCMC complete!\n",
      "--- duplicates: 103/1000  |  highest count: 14x\n",
      "[saved intermediate result]\n",
      "Tempering finished!\n",
      "Tempering steps done:  6\n"
     ]
    }
   ],
   "source": [
    "### M:  number of particles/samples for SMC\n",
    "num_samples = 1000\n",
    "\n",
    "### var: uncertainty variance of measurements/modeling\n",
    "std_noise = 0.05\n",
    "\n",
    "### Title for saving the results\n",
    "run = \"acousticgravity\"  # or any identifier you want\n",
    "resultspath = f'RESULTS/SMC_M_{num_samples}_{run}'\n",
    "\n",
    "# Wrapper for scipy prior\n",
    "class prior:\n",
    "    \"\"\"\n",
    "    Class for the prior distribution, here for uniform\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, prior, d = 1):\n",
    "        self.d = d # dimension of prior/parameter space, here 1\n",
    "        self.RV = prior # this is specific to this prior\n",
    "\n",
    "    def rvs(self, M):\n",
    "        # Generate n samples. The resulting numpy array must be [1:d, 1:M]\n",
    "        return np.array([self.RV.rvs(size=M) for i in range(self.d)])\n",
    "\n",
    "    def logpdf(self, particle):\n",
    "        # Compute logpdf at a point \"particle\"\n",
    "        return np.sum([self.RV.logpdf(particle[i]) for i in range(self.d)])\n",
    "    \n",
    "a = 1.\n",
    "b = 4.\n",
    "my_prior = prior(stats.uniform(a,b))\n",
    "    \n",
    "### Set up forward model\n",
    "def forward_model(x0):\n",
    "    t_measurements = np.array([0.1,0.3,0.5,0.8, 1.]) # example: data are measurements at different times\n",
    "    a = 1.    \n",
    "    return x0*np.exp(-a*t_measurements)\n",
    "\n",
    "def potential(particle, data, std_noise = 0.05):\n",
    "    fwd_vals = forward_model(particle)\n",
    "    return -1.*(np.linalg.norm(fwd_vals-data)**2/(2.*std_noise**2))\n",
    "\n",
    "# Multiprocessing evaluation of previous potential \n",
    "def potential_mp(particles,data, std_noise = 0.05):\n",
    "    pool = mp.Pool()\n",
    "    potentials = pool.map(ft.partial(potential, data=data, std_noise=std_noise),particles.T)\n",
    "    pool.close()\n",
    "    return np.array(potentials)\n",
    "\n",
    "### Generate synthetic data\n",
    "prior_sample = my_prior.rvs(1)\n",
    "print(\"Truth = \", prior_sample)\n",
    "t_measurements = np.array([0.1,0.3,0.5,0.8, 1.]) # example: data are measurements at different times\n",
    "forward_model_values = forward_model(prior_sample) \n",
    "data = forward_model_values + np.random.normal(0.,std_noise)\n",
    "print(\"Noiseless data\", forward_model_values)\n",
    "print(\"Data\", data)\n",
    "\n",
    "### SET UP SMC SAMPLER  \n",
    "SMC_sampler = SMC(my_prior, lambda x: potential_mp(x, data = data, std_noise = std_noise), resultspath, data, num_samples)\n",
    "SMC_sampler.smc_tempering() ### perform SMC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
